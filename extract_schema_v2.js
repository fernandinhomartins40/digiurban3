// ====================================================================
// üîç EXTRATOR DE SCHEMA USANDO API REST
// ====================================================================

import { createClient } from '@supabase/supabase-js'
import fs from 'fs'
import path from 'path'

// Configura√ß√£o do Supabase
const supabaseUrl = 'https://warbeochfoabfptvvtpu.supabase.co'
const serviceRoleKey = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6IndhcmJlb2NoZm9hYmZwdHZ2dHB1Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1NTUzMzk5MSwiZXhwIjoyMDcxMTA5OTkxfQ.EfuTEme81CQGXxhSFVFMBkvU4HkwlQ8le24AHUh8Jho'

// Criar cliente Supabase
const supabase = createClient(supabaseUrl, serviceRoleKey, {
  auth: {
    autoRefreshToken: false,
    persistSession: false
  }
})

// Diret√≥rio de sa√≠da
const outputDir = 'supabase/sql_dump'

// Criar diret√≥rio se n√£o existir
if (!fs.existsSync(outputDir)) {
  fs.mkdirSync(outputDir, { recursive: true })
}

console.log('üöÄ Iniciando extra√ß√£o do schema via API REST...\n')

async function extractTables() {
  try {
    console.log('üìã Extraindo lista de tabelas...')
    
    // Tentar usar a API REST para obter metadados
    const response = await fetch(`${supabaseUrl}/rest/v1/`, {
      headers: {
        'apikey': serviceRoleKey,
        'Authorization': `Bearer ${serviceRoleKey}`
      }
    })
    
    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`)
    }
    
    const apiInfo = await response.text()
    console.log('‚úÖ Conex√£o estabelecida com a API')
    
    // Vamos extrair informa√ß√µes das tabelas que sabemos que existem
    const knownTables = [
      'tenants',
      'user_profiles', 
      'secretarias',
      'protocolos'
    ]
    
    const schemaInfo = {
      extraction_date: new Date().toISOString(),
      database_url: supabaseUrl,
      tables: {}
    }
    
    for (const tableName of knownTables) {
      try {
        console.log(`üìä Extraindo estrutura da tabela: ${tableName}`)
        
        // Tentar obter dados da tabela (limitado) para ver estrutura
        const { data, error } = await supabase
          .from(tableName)
          .select('*')
          .limit(1)
        
        if (error) {
          console.log(`‚ö†Ô∏è Erro ao acessar ${tableName}:`, error.message)
          schemaInfo.tables[tableName] = {
            error: error.message,
            accessible: false
          }
        } else {
          console.log(`‚úÖ Tabela ${tableName} acess√≠vel`)
          schemaInfo.tables[tableName] = {
            accessible: true,
            sample_data: data,
            columns: data && data.length > 0 ? Object.keys(data[0]) : []
          }
        }
        
      } catch (err) {
        console.log(`‚ùå Erro na tabela ${tableName}:`, err.message)
        schemaInfo.tables[tableName] = {
          error: err.message,
          accessible: false
        }
      }
    }
    
    // Salvar informa√ß√µes coletadas
    fs.writeFileSync(
      path.join(outputDir, '01_schema_info.json'),
      JSON.stringify(schemaInfo, null, 2)
    )
    
    console.log('\n‚úÖ Informa√ß√µes do schema extra√≠das!')
    return schemaInfo
    
  } catch (error) {
    console.error('‚ùå Erro na extra√ß√£o:', error)
    return null
  }
}

async function extractFromMigrations() {
  try {
    console.log('\nüìÅ Lendo arquivos de migra√ß√£o locais...')
    
    const migrationsDir = 'supabase/migrations'
    const schemaCompleto = {}
    
    if (fs.existsSync(migrationsDir)) {
      const files = fs.readdirSync(migrationsDir)
      
      for (const file of files) {
        if (file.endsWith('.sql')) {
          console.log(`üìÑ Lendo: ${file}`)
          const content = fs.readFileSync(path.join(migrationsDir, file), 'utf8')
          
          // Copiar para sql_dump
          fs.writeFileSync(
            path.join(outputDir, `migration_${file}`),
            content
          )
          
          schemaCompleto[file] = {
            size: content.length,
            lines: content.split('\n').length,
            content: content.substring(0, 1000) + '...' // Primeiro KB
          }
        }
      }
    }
    
    fs.writeFileSync(
      path.join(outputDir, '02_migrations_summary.json'),
      JSON.stringify(schemaCompleto, null, 2)
    )
    
    console.log('‚úÖ Migra√ß√µes processadas!')
    
  } catch (error) {
    console.error('‚ùå Erro ao processar migra√ß√µes:', error)
  }
}

async function createConsolidatedSchema() {
  try {
    console.log('\nüìã Criando schema consolidado...')
    
    const schemaContent = `-- ====================================================================
-- SCHEMA COMPLETO DO DIGIURBAN2 - BACKUP EXTRA√çDO
-- ====================================================================
-- Data de extra√ß√£o: ${new Date().toISOString()}
-- Projeto: warbeochfoabfptvvtpu
-- URL: ${supabaseUrl}
-- ====================================================================

-- Este arquivo foi gerado automaticamente atrav√©s da extra√ß√£o do banco
-- Cont√©m a estrutura principal conhecida do sistema DigiUrban2

-- EXTENS√ïES NECESS√ÅRIAS
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";
CREATE EXTENSION IF NOT EXISTS "unaccent";

-- TIPOS ENUM CONHECIDOS
CREATE TYPE status_base_enum AS ENUM ('ativo', 'inativo', 'pendente', 'suspenso');
CREATE TYPE status_tenant_enum AS ENUM ('ativo', 'suspenso', 'cancelado', 'trial');
CREATE TYPE status_processo_enum AS ENUM ('aberto', 'em_andamento', 'aguardando_documentos', 'aguardando_aprovacao', 'aprovado', 'rejeitado', 'concluido', 'cancelado', 'suspenso', 'em_revisao');
CREATE TYPE status_agendamento_enum AS ENUM ('agendado', 'confirmado', 'em_andamento', 'concluido', 'cancelado', 'nao_compareceu', 'reagendado');
CREATE TYPE prioridade_enum AS ENUM ('baixa', 'media', 'alta', 'urgente', 'critica');
CREATE TYPE user_tipo_enum AS ENUM ('super_admin', 'admin', 'secretario', 'diretor', 'coordenador', 'supervisor', 'operador', 'cidadao');
CREATE TYPE tenant_plano_enum AS ENUM ('starter', 'professional', 'enterprise');

-- TABELAS PRINCIPAIS
-- (As defini√ß√µes completas est√£o nos arquivos de migra√ß√£o)

-- NOTA: Para o schema completo e detalhado, consulte os arquivos:
-- - migration_*.sql (defini√ß√µes completas das tabelas)
-- - *_info.json (metadados extra√≠dos)

-- ====================================================================
-- FIM DO SCHEMA CONSOLIDADO
-- ====================================================================
`
    
    fs.writeFileSync(
      path.join(outputDir, 'schema_completo_consolidado.sql'),
      schemaContent
    )
    
    console.log('‚úÖ Schema consolidado criado!')
    
  } catch (error) {
    console.error('‚ùå Erro ao criar schema consolidado:', error)
  }
}

// ====================================================================
// EXECUTAR EXTRA√á√ÉO COMPLETA
// ====================================================================
async function runExtraction() {
  console.log('üöÄ Iniciando extra√ß√£o completa do schema DigiUrban2\n')
  
  // 1. Extrair via API
  await extractTables()
  
  // 2. Processar migra√ß√µes locais
  await extractFromMigrations()
  
  // 3. Criar schema consolidado
  await createConsolidatedSchema()
  
  console.log('\nüéâ Extra√ß√£o completa finalizada!')
  console.log(`üìÅ Todos os arquivos salvos em: ${outputDir}/`)
  console.log('\nüìã Arquivos gerados:')
  console.log('   - 01_schema_info.json (metadados das tabelas)')
  console.log('   - 02_migrations_summary.json (resumo das migra√ß√µes)')
  console.log('   - migration_*.sql (arquivos de migra√ß√£o copiados)')
  console.log('   - schema_completo_consolidado.sql (schema principal)')
}

runExtraction().catch(console.error)